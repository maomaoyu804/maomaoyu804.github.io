<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="浅浅看了一小部分复旦大学邱锡鹏老师的《神经网络与深度学习》书：https:&#x2F;&#x2F;nndl.github.io视频：https:&#x2F;&#x2F;www.bilibili.com&#x2F;video&#x2F;BV13b4y1177W?spm_id_from&#x3D;333.999.0.0本文是从notion搬过来的，有很多格式问题待处理，coming soon……">
<meta property="og:type" content="article">
<meta property="og:title" content="[nlp]神经网络与深度学习">
<meta property="og:url" content="http://maomaoyu804.github.io/2022/02/17/Untitled/index.html">
<meta property="og:site_name" content="毛毛雨">
<meta property="og:description" content="浅浅看了一小部分复旦大学邱锡鹏老师的《神经网络与深度学习》书：https:&#x2F;&#x2F;nndl.github.io视频：https:&#x2F;&#x2F;www.bilibili.com&#x2F;video&#x2F;BV13b4y1177W?spm_id_from&#x3D;333.999.0.0本文是从notion搬过来的，有很多格式问题待处理，coming soon……">
<meta property="og:locale">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/1.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/2.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/3.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/4.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/5.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/6.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/7.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/8.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/9.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/10.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/11.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/12.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/13.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/14.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/15.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/16.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/17.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/18.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/19.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/20.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/21.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/22.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/23.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/24.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/25.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/26.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/27.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/28.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/29.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/30.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/31.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/32.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/33.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/34.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/35.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/36.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/37.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/38.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/39.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/40.png">
<meta property="og:image" content="http://maomaoyu804.github.io/images/nlp/41.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f5cc53b1-62ed-486c-b80c-cd23e636385d/截屏2022-01-26_下午1.41.40.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/018bf558-4642-4d7e-b341-63a6202a3a72/截屏2022-01-26_下午1.48.32.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6f31f9b8-156e-42bd-aa62-3b16f36a3b1b/截屏2022-01-26_下午1.57.30.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c1b4a0c2-0b8b-467d-b86e-219d68408db0/截屏2022-01-26_下午1.58.16.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4f9c91f7-879f-4083-aae6-49a34671a9bf/截屏2022-01-26_下午2.08.32.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/79aa1bfb-3025-449e-a039-633ca7ffb955/截屏2022-01-26_下午2.07.18.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/815dc84b-ae25-4280-9e62-8e9993392073/截屏2022-01-26_下午3.01.32.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d6bd6bb1-ffa2-4e6b-b33f-fc44cfa2c270/截屏2022-01-26_下午3.10.48.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ec353c9e-700f-4477-8c9f-5beca6457f7b/截屏2022-01-26_下午3.14.23.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/72317b9b-5ce0-4451-8b1f-47c12d282af4/截屏2022-01-26_下午5.09.03.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a63a3f3b-76cf-43f6-8d07-928d88243080/截屏2022-01-26_下午5.21.40.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4ba697b1-3d21-4a3c-8c8c-6981ade42dd5/截屏2022-01-26_下午5.35.34.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d9ae9ab0-a510-451a-a820-f2829bc180e1/截屏2022-01-26_下午5.38.24.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3e5c35ae-a338-481a-8b72-20f57812e4a9/截屏2022-01-26_下午5.53.15.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a428bd44-6289-49ec-a0f2-42bce7aa2d8c/截屏2022-01-26_下午6.11.29.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e93b75c4-162b-4305-9c8d-27163f241d43/截屏2022-01-26_下午6.29.16.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/04e18b08-4f1b-4ce8-be35-e5f0b7e8cf17/截屏2022-01-26_下午6.33.25.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ed985810-b44f-4c6e-933b-b7101d10bf03/截屏2022-01-26_下午6.34.06.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f4aace8d-1649-488c-8b54-bbc74f8808b7/截屏2022-01-26_下午6.34.45.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4555b64f-1914-4a4a-8f64-2f48ad1882cc/截屏2022-01-26_下午6.42.32.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fe1a6084-e610-4f8c-8cde-fd9c27b789f8/截屏2022-01-26_下午6.45.02.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6044162e-1b54-401c-8e7e-9ac7e61160aa/截屏2022-01-26_下午6.46.32.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e08d3d42-dd4c-4c63-a4bd-10eb4017621a/截屏2022-01-26_下午6.47.49.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0dc95cd6-880b-424e-b35e-2eb956d22678/截屏2022-01-27_下午4.18.19.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3c39f648-9f6e-44eb-8416-5010c6069432/截屏2022-01-27_下午4.28.31.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6cb97203-7a54-4e67-aa99-51b44e5bf06c/截屏2022-01-27_下午4.33.57.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f2b7fbaa-6126-4182-8dc0-dce6e652a7ab/截屏2022-01-27_下午5.08.49.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d6328e2d-47b5-4ed8-bf50-76361dc297c6/截屏2022-01-27_下午5.11.35.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/27792003-9b23-4867-84a2-393b8e526944/截屏2022-01-27_下午5.14.14.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b54be2e1-f94e-441f-a17f-82da079f3cf1/截屏2022-01-27_下午5.19.48.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6dbb6ee9-ab15-46c2-aa76-2042824170d1/截屏2022-01-27_下午5.26.36.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b98915ad-e73d-49c3-a75f-c215e00de0fa/截屏2022-01-27_下午7.52.41.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9625b520-a84a-4b52-8f8a-560c85398087/截屏2022-01-27_下午8.05.41.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/83d81c83-b13e-4c6c-aaeb-bd5fa86327de/截屏2022-01-27_下午8.06.47.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5c4f17fa-7bf5-447e-a244-0a1c0e0ee6c8/截屏2022-01-28_下午1.44.09.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/35e91ed2-8258-4b9f-94d4-bfc5830da6bb/截屏2022-01-28_下午1.51.01.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/dc0df44e-7a8f-4e0e-bdbe-639a6581a1b8/截屏2022-01-28_下午2.00.06.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ed2a0d80-b8c8-41e6-8c1d-24ca8a5ad8fe/截屏2022-01-28_下午2.04.11.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0d3dbcd5-b317-4362-9482-e34669534d1f/截屏2022-01-28_下午2.06.37.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/23758915-dcc6-42e7-a8a9-5d7099f0c3dd/截屏2022-01-28_下午2.19.33.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e83bf90a-4161-4e7f-ab37-f01cb28cb05c/截屏2022-01-28_下午2.21.34.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1238e591-7560-4256-8ab9-e675c88bfa75/截屏2022-01-28_下午2.23.33.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/51eff151-6a04-4914-9763-4462dbf5188a/截屏2022-01-28_下午2.25.46.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/efe6d951-4964-4618-88c5-a023b71d4982/截屏2022-01-28_下午3.21.18.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fe20e161-3850-49d6-86ed-04d7b87019b0/截屏2022-01-28_下午3.24.10.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3f47959d-5721-4d33-b8db-c6e9de81cab8/截屏2022-01-28_下午3.26.07.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1552232d-04ba-47d3-b051-8d6e5be8bc70/截屏2022-01-28_下午3.27.05.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9afc56ea-7304-49d9-ab47-68ececa545ab/截屏2022-01-28_下午3.27.52.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2214601c-ead2-46cf-a7b7-08ca3f39ce04/截屏2022-01-28_下午3.44.32.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8e3b4290-b32b-4bb9-bcec-b9c0836d6f5a/截屏2022-01-28_下午3.45.09.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e7e5a3a1-7429-41eb-a030-f5ca8311385f/截屏2022-01-28_下午4.05.32.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/123516b3-7bff-4d07-a204-d0a31122c3d9/截屏2022-01-28_下午4.07.04.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/7b6b96e6-b6c6-4a6c-93bb-c7d8b0bf6d0d/截屏2022-01-28_下午4.08.07.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ca8d8928-43f5-49bc-83ed-874f47f4a0b5/截屏2022-01-28_下午4.10.21.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e3d74abb-2887-474c-b162-282ff88a4560/截屏2022-01-28_下午4.12.59.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/80623b51-306c-4895-9131-c37ec73c6b58/截屏2022-01-28_下午4.12.42.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/7de8ebdc-78ef-4d66-b120-da8d2fc7a6cb/截屏2022-01-28_下午4.15.30.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/016fc21f-3a8d-4617-a823-b69113430d01/截屏2022-01-28_下午4.16.43.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/03b7494c-2cdd-4929-90d0-cddcacfbc29c/截屏2022-01-28_下午4.23.10.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8193a56d-a37e-4dde-a0c7-8a8db4096210/截屏2022-01-28_下午4.43.09.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/28278475-3541-41b1-9d01-d33eb18f604b/截屏2022-01-28_下午4.52.36.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d9b9acf2-32af-4b5c-abb5-b49854bf06e3/截屏2022-01-28_下午4.58.40.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/476a79cf-92ef-49d7-85f0-9e326a784032/截屏2022-01-28_下午5.04.53.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b1976804-4a2d-487d-aaa9-b3edfc9da07d/截屏2022-01-28_下午5.08.18.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a945fb8b-d9c8-40fa-8ba3-995e4c1cd41c/截屏2022-01-28_下午5.09.01.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/02e30255-9ca5-4c21-a902-1c8052f605e2/截屏2022-01-28_下午5.18.06.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a35493c2-897f-4733-9829-fe5b09b5aa0d/截屏2022-01-28_下午5.26.12.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/58f3ee42-a79f-4209-947a-e3e96bc4b8e6/截屏2022-01-29_下午4.15.32.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6cd95c69-9187-4773-81ef-fc36f4492965/截屏2022-01-29_下午4.16.11.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3482ffb7-947b-4609-badb-108ffb8de48a/截屏2022-01-29_下午5.20.28.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ca325b68-01b2-49b4-a819-2e51cee93dbe/截屏2022-01-29_下午5.30.31.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9736b983-0e39-40f4-8c84-b80a5be604f6/截屏2022-01-29_下午5.33.08.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1bfcc5c8-a944-4f26-a395-c22309a807cc/截屏2022-01-29_下午5.33.44.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/bc65b87c-4977-4f02-b686-bf6259d14645/截屏2022-01-29_下午5.38.29.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/114fa287-215a-4167-9bee-4cdd3cc44bcd/截屏2022-01-29_下午5.41.31.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1521b54c-fe94-485d-a6e1-7c69d64b82a5/截屏2022-01-29_下午11.08.56.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2579229c-4e37-468c-9333-cdc722ada73e/截屏2022-01-29_下午11.09.57.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6914430c-a5f1-4f30-b497-c256f4cfd48b/截屏2022-01-29_下午11.14.57.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/051257eb-a473-47c1-b13c-61047be3546c/截屏2022-01-29_下午11.16.47.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a5c20dcd-d107-491f-92b3-558de0e994fb/截屏2022-01-29_下午11.20.23.png">
<meta property="article:published_time" content="2022-02-17T13:54:38.000Z">
<meta property="article:modified_time" content="2022-02-17T14:38:02.532Z">
<meta property="article:author" content="毛毛雨 &#x3D;)">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://maomaoyu804.github.io/images/nlp/1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://maomaoyu804.github.io/2022/02/17/Untitled/"/>





  <title>[nlp]神经网络与深度学习 | 毛毛雨</title>
  








<meta name="generator" content="Hexo 6.0.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">毛毛雨</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">(^_−)−☆</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://maomaoyu804.github.io/2022/02/17/Untitled/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="毛毛雨">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">[nlp]神经网络与深度学习</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2022-02-17T21:54:38+08:00">
                2022-02-17
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2022-02-17T22:38:02+08:00">
                2022-02-17
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>浅浅看了一小部分复旦大学邱锡鹏老师的《神经网络与深度学习》<br>书：<a target="_blank" rel="noopener" href="https://nndl.github.io">https://nndl.github.io</a><br>视频：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV13b4y1177W?spm_id_from=333.999.0.0">https://www.bilibili.com/video/BV13b4y1177W?spm_id_from=333.999.0.0</a><br>本文是从notion搬过来的，有很多格式问题待处理，coming soon……<br><span id="more"></span></p>
<p><img src="/images/nlp/1.png" alt=""></p>
<h1 id="第一章-绪论"><a href="#第一章-绪论" class="headerlink" title="第一章 绪论"></a>第一章 绪论</h1><blockquote>
<p>一个人在不接触对方的情况下，通过一种特殊的方式和对方进行一系列的问答，如果在相当长时间内，他无法根据这些问题判断对方是人还是计算机，那么就可以认为这个计算机是智能的</p>
</blockquote>
<h2 id="1-1-人工智能"><a href="#1-1-人工智能" class="headerlink" title="1.1 人工智能"></a>1.1 人工智能</h2><p><strong>主要领域</strong><br>（1）感知：计算机视觉、语音信息处理等<br>（2）学习：监督学习、无监督学习、强化学习等<br>（3）认知：知识表示、自然语言处理、推理、规划、决策等</p>
<h3 id="1-1-1-人工智能的发展历史"><a href="#1-1-1-人工智能的发展历史" class="headerlink" title="1.1.1 人工智能的发展历史"></a>1.1.1 人工智能的发展历史</h3><p><img src="/images/nlp/2.png" alt=""></p>
<h3 id="1-1-2-人工智能的流派"><a href="#1-1-2-人工智能的流派" class="headerlink" title="1.1.2 人工智能的流派"></a>1.1.2 人工智能的流派</h3><p><strong>符号主义</strong><br>通过分析人类智能的功能，用计算机来实现<br>两个基本假设：（1）信息可以用符号表示（2）符号可以通过显式规则（如逻辑运算）来操作<br>可解释性<br><strong>连接主义</strong><br>人类的认识过程是由大量简单神经元构成的神经网络中的信息处理过程，而不是符号运算<br>特性：非线性、分布式、并行化、局部性计算、自适应性  </p>
<h2 id="1-2-机器学习（ML）"><a href="#1-2-机器学习（ML）" class="headerlink" title="1.2 机器学习（ML）"></a>1.2 机器学习（ML）</h2><p><img src="/images/nlp/3.png" alt=""><br>浅层学习：</p>
<ul>
<li>不涉及特征学习，主要靠人工经验或特征转换方法抽取  </li>
</ul>
<p>特征转换：  </p>
<ul>
<li>降维（特征抽取、特征选择）、升维  </li>
<li>方法：主成分分析（PCA）、线性判别分析（LDA）  </li>
<li>前三步十分关键，因此许多机器学习问题都变成了特征工程问题。  </li>
</ul>
<h2 id="1-3-表示学习"><a href="#1-3-表示学习" class="headerlink" title="1.3 表示学习"></a>1.3 表示学习</h2><p>表示学习：自动学习有效的特征，并提高最终机器学习模型的性能<br>语义鸿沟：输入数据的<strong>底层特征</strong>和<strong>高层语义</strong>之间的不一致性和差异性<br>表示学习的核心问题：什么是一个好的表示、如何学习到好的表示</p>
<h3 id="1-3-1-局部表示和分布式表示"><a href="#1-3-1-局部表示和分布式表示" class="headerlink" title="1.3.1 局部表示和分布式表示"></a>1.3.1 局部表示和分布式表示</h3><p><img src="/images/nlp/4.png" alt=""><br><img src="/images/nlp/5.png" alt=""></p>
<h3 id="1-3-2-表示学习"><a href="#1-3-2-表示学习" class="headerlink" title="1.3.2 表示学习"></a>1.3.2 表示学习</h3><p>要学习到一种好的高层语义表示(一般为分布式表示)，通常需要从<strong>底层特征</strong>开始，经过多步<strong>非线性转换</strong>才能得到。<strong>深层结构</strong>的优点是可以增加特征的重用性，从而指数级地增加表示能力。因此，表示学习的<strong>关键</strong>是构建具有<strong>一定深度的多层次特征表示</strong>。</p>
<h2 id="1-4-深度学习（DL）"><a href="#1-4-深度学习（DL）" class="headerlink" title="1.4 深度学习（DL）"></a>1.4 深度学习（DL）</h2><p>深度：非线性特征转换次数<br><img src="/images/nlp/6.png" alt=""><br>贡献度分配问题（CAP）：一个系统中不同的<strong>组件</strong>或其参数对最终系统输出结果的贡献或影响，深度学习需要解决的关键问题。<br>深度学习：主要采用神经网络模型，反向传播解决贡献度分配问题<br>端到端学习：在学习过程中不进行分模块或分阶段训练，直接优化任务的总体目标。</p>
<h2 id="1-5-神经网络"><a href="#1-5-神经网络" class="headerlink" title="1.5 神经网络"></a>1.5 神经网络</h2><h3 id="1-5-1-人脑神经网络"><a href="#1-5-1-人脑神经网络" class="headerlink" title="1.5.1 人脑神经网络"></a>1.5.1 人脑神经网络</h3><p><img src="/images/nlp/7.png" alt=""><br>短期记忆、长期记忆</p>
<h3 id="1-5-2-人工神经网络"><a href="#1-5-2-人工神经网络" class="headerlink" title="1.5.2 人工神经网络"></a>1.5.2 人工神经网络</h3><p>人工神经网络与生物神经元类似，由多个节点(人工神经元)互相连接而成，可以用来对数据之间的复杂关系进行建模。<br>不同节点之间的连接被赋予了不同的权重，每个权重代表了一个节点对另一个节点的影响大小。<br>每个节点代表一种特定函数，来自其他节点的信息经过其相应的权重综合计算，输入到一个激活函数中并得到一个新的活性值(兴奋或抑制)。<br>从系统观点看，人工神经元网络是由大量神经元通过极其丰富和完善的连接而构成的自适应非线性动态系统。</p>
<h3 id="1-5-3-神经网络的发展历史"><a href="#1-5-3-神经网络的发展历史" class="headerlink" title="1.5.3 神经网络的发展历史"></a>1.5.3 神经网络的发展历史</h3><h2 id="1-6-本书的知识体系"><a href="#1-6-本书的知识体系" class="headerlink" title="1.6 本书的知识体系"></a>1.6 本书的知识体系</h2><p><img src="/images/nlp/8.png" alt=""></p>
<h1 id="第二章-机器学习概述"><a href="#第二章-机器学习概述" class="headerlink" title="第二章 机器学习概述"></a>第二章 机器学习概述</h1><h2 id="2-1-基本概念"><a href="#2-1-基本概念" class="headerlink" title="2.1 基本概念"></a>2.1 基本概念</h2><p><img src="/images/nlp/9.png" alt=""><br><img src="/images/nlp/10.png" alt=""></p>
<h2 id="2-2-机器学习的三个基本要素"><a href="#2-2-机器学习的三个基本要素" class="headerlink" title="2.2 机器学习的三个基本要素"></a>2.2 机器学习的三个基本要素</h2><p>模型、学习准则、优化算法</p>
<h3 id="2-2-1-模型"><a href="#2-2-1-模型" class="headerlink" title="2.2.1 模型"></a>2.2.1 模型</h3><p>找到一个模型来近似真实映射函数 𝑔(𝒙) 或真实条件概率分布 𝑝𝑟 (𝑦|𝒙).</p>
<p><img src="/images/nlp/11.png" alt=""><br><strong>线性模型</strong><br><img src="/images/nlp/12.png" alt=""><br><strong>非线性模型</strong><br><img src="/images/nlp/13.png" alt=""><br><img src="/images/nlp/14.png" alt=""></p>
<h3 id="2-2-2-学习准则"><a href="#2-2-2-学习准则" class="headerlink" title="2.2.2 学习准则"></a>2.2.2 学习准则</h3><p><img src="/images/nlp/15.png" alt=""><br><strong>损失函数</strong></p>
<ul>
<li>0-1损失函数<br>  <img src="/images/nlp/16.png" alt=""><br>  缺点：数学性质不好：不连续且导数为0，难以优化</li>
<li>平方损失函数<br>  <img src="/images/nlp/17.png" alt=""><br>  常用于在预测标签y为实数值的情况下，不适用于分类问题</li>
<li>交叉熵损失函数（负对数似然函数）<br>  <img src="/images/nlp/18.png" alt=""><br>  用于分类问题，且概率和为1</li>
<li>Hinge损失函数<br>  <img src="/images/nlp/19.png" alt=""><br>  <img src="/images/nlp/20.png" alt=""><br>  用于二分类问题</li>
</ul>
<p><strong>风险最小化准则</strong></p>
<ul>
<li>经验风险最小化（ERM）<br>  在训练集上的平均损失：<br>  <img src="/images/nlp/21.png" alt=""><br>  学习准则找到一组参数$\theta^*$使得经验风险最小<br>  <img src="/images/nlp/22.png" alt=""></li>
<li>结构风险最小化（SRM）<br>  防止过拟合<br>  <img src="/images/nlp/23.png" alt=""><br>  $\lambda$控制正则化强度<br>  <img src="/images/nlp/24.png" alt=""></li>
</ul>
<h3 id="2-2-3-优化算法"><a href="#2-2-3-优化算法" class="headerlink" title="2.2.3 优化算法"></a>2.2.3 优化算法</h3><p>参数：通过优化算法进行学习<br>超参数：定义模型结构或优化策略等，如：聚类算法中类别个数、梯度下降法中的步长等<br><strong>梯度下降法</strong><br><img src="/images/nlp/25.png" alt=""><br><strong>提前停止</strong><br><img src="/images/nlp/26.png" alt=""><br><strong>随机梯度下降法</strong><br>批量梯度下降法（BGD）：目标函数是整个训练集上的风险函数<br>随机梯度下降法（SGD）：每次迭代时只采集一个样本<br>小批量梯度下降法：每次迭代选用小部分训练样本  </p>
<h2 id="2-3-机器学习的简单示例——线性回归"><a href="#2-3-机器学习的简单示例——线性回归" class="headerlink" title="2.3 机器学习的简单示例——线性回归"></a>2.3 机器学习的简单示例——线性回归</h2><p>参数学习</p>
<h3 id="从函数关系看"><a href="#从函数关系看" class="headerlink" title="从函数关系看"></a>从函数关系看</h3><p><strong>经验风险最小化</strong></p>
<script type="math/tex; mode=display">
\mathcal{R}(\boldsymbol w)=\sum_{n=1}^N \mathcal{L}(y^{(n)},f(\boldsymbol x^{(n)};\boldsymbol w))\\
=\frac{1}{2}\sum_{n=1}^N (y^{(n)}-\boldsymbol w^T\boldsymbol x^{(n)}))^2\\
=\frac{1}{2}||y-\boldsymbol X^T\boldsymbol w||^2</script><p>具体推导见书P34<br>最小二乘法（LSM）：</p>
<script type="math/tex; mode=display">
w^*=(\boldsymbol X\boldsymbol X^T)^{-1}\boldsymbol X\boldsymbol y</script><p>当$\boldsymbol X\boldsymbol X^T$不可逆时，估计参数的方法：</p>
<ol>
<li>主成分分析等方法预处理数据，消除特征之间的相关性，再使用最小二乘法</li>
<li>最小均方算法（LMS）：梯度下降法估计参数，初始$\boldsymbol w=0$<br><strong>结构风险最小化</strong><br>特征之间有比较大的多重共线性<br>岭回归:<script type="math/tex; mode=display">
\boldsymbol w^*=(\boldsymbol X\boldsymbol X^T+\lambda \boldsymbol I)^{-1}\boldsymbol X\boldsymbol y</script>即给$XX^T$对角线元素加一个常数，使其行列式不为零<script type="math/tex; mode=display">
\mathcal{R}(\boldsymbol w)
=\frac{1}{2}||\boldsymbol y-\boldsymbol X^T\boldsymbol w||^2+\frac{1}{2}\lambda ||\boldsymbol w||^2</script>其中$\lambda &gt;0$为正则化系数</li>
</ol>
<h3 id="概率视角看线性回归"><a href="#概率视角看线性回归" class="headerlink" title="概率视角看线性回归"></a>概率视角看线性回归</h3><p><strong>最大似然估计(MLE)</strong></p>
<script type="math/tex; mode=display">
y=f(\boldsymbol x;\boldsymbol w)+\epsilon</script><p>添加噪声$\epsilon$服从均值为0，方差为$\sigma^2$的高斯分布，于是$y$服从均值为$w^Tx$，方差为$\sigma^2$的高斯分布：<br><img src="/images/nlp/27.png" alt=""><br>y就变成了一个随机变量<br>似然函数：<br><img src="/images/nlp/28.png" alt=""><br>机器学习中通常求解w，因此看作关于w的函数，用似然函数<br><img src="/images/nlp/29.png" alt=""><br>最大似然估计：找到参数w，使得似然函数最大<br>似然函数通常和指数相关，取log方便计算 </p>
<script type="math/tex; mode=display">
\frac{\partial \log p(y|\boldsymbol X;\boldsymbol w,\sigma)}{\partial \boldsymbol w}=0\\
\boldsymbol {w^{ML}}=(\boldsymbol X\boldsymbol X^T)^{-1}\boldsymbol X\boldsymbol y</script><p>给定$\boldsymbol x^{(n)}$时，正确的样本概率最大<br>最大似然估计的解等价于线性回归的经验风险最小化的解<br><strong>贝叶斯学习（后验分布）</strong><br><img src="/images/nlp/30.png" alt=""><br>贝叶斯公式： $p(y|x)=\frac{p(x,y)}{p(x)}=\frac{p(x|y)p(y)}{p(x)}$<br>贝叶斯回归：用贝叶斯估计的线性回归<br><img src="/images/nlp/31.png" alt=""><br>类似线性回归结构风险最小化的解</p>
<script type="math/tex; mode=display">
y=E_{N\sim P(\boldsymbol w|\boldsymbol X,\boldsymbol y;v,\sigma)}[f(\boldsymbol x,\boldsymbol w)]</script><p>这里估计出来的$P$是个分布<br><strong>最大后验估计（MAP）</strong><br><img src="/images/nlp/32.png" alt=""><br><img src="/images/nlp/33.png" alt=""><br>找到一个$\boldsymbol w$，使得$\boldsymbol  w$的后验最大<br>对应结构风险最小化<br>结构风险最小化引入的是正则化项，最大后验估计中引入的是先验分布，统一成为都引入先验（人的经验）：第一个是限制w的取值范围，第二个是w服从以0为中心的高斯分布<br><img src="/images/nlp/34.png" alt=""></p>
<h2 id="多项式曲线拟合"><a href="#多项式曲线拟合" class="headerlink" title="多项式曲线拟合"></a><strong>多项式曲线拟合</strong></h2><p><img src="/images/nlp/35.png" alt=""><br>其中$\phi(x)=[1,x,x^2,…,x^M]^T$</p>
<script type="math/tex; mode=display">
\mathcal{R}(w)=\frac{1}{2}\sum_{n=1}^N (y^{(n)}-w^T\phi (x^{(n)}))^2</script><script type="math/tex; mode=display">
w^*=</script><p>模型选择问题（对于超参数M）：<br><img src="/images/nlp/36.png" alt=""><br>控制过拟合：正则化或增加训练样本数量</p>
<h2 id="2-4-偏差-方差分解"><a href="#2-4-偏差-方差分解" class="headerlink" title="2.4 偏差-方差分解"></a>2.4 偏差-方差分解</h2><p><img src="/images/nlp/37.png" alt=""><br><img src="/images/nlp/38.png" alt=""><br> 偏差：均值离f<em>的距离，方差是这三个f的远近<br><img src="/images/nlp/39.png" alt=""><br>偏差+方差+噪声<br><img src="/images/nlp/40.png" alt=""><br>蓝点是不同训练集D上得到的模型f，中间是最优模型f</em><br>a: 理想情况<br>b: 欠拟合：偏差大方差小，泛化能力好，但拟合能力不足<br>c: 过拟合：方差大偏差小拟合能力好，但泛化能力不足<br><img src="/images/nlp/41.png" alt=""><br>集成模型：有效降低方差的方法（通过多个高方差模型的平均降低方差）</p>
<h2 id="2-5-定理"><a href="#2-5-定理" class="headerlink" title="2.5 定理"></a>2.5 定理</h2><p>没有免费午餐定理<br>丑小鸭定理<br>奥卡姆剃刀定理<br>归纳偏置（先验）<br>PAC学习：训练集大小趋向于无穷大时，泛化误差趋向于0，即经验风险趋近于期望风险</p>
<h1 id="第三章-线性模型"><a href="#第三章-线性模型" class="headerlink" title="第三章 线性模型"></a>第三章 线性模型</h1><h2 id="3-1-分类问题示例"><a href="#3-1-分类问题示例" class="headerlink" title="3.1 分类问题示例"></a>3.1 分类问题示例</h2><p>图像分类、目标检测、实例分割（像素级分类问题）<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f5cc53b1-62ed-486c-b80c-cd23e636385d/截屏2022-01-26_下午1.41.40.png" alt="截屏2022-01-26 下午1.41.40.png"><br>垃圾邮件过滤<br>文档归类<br>情感分类<br>文本分类</p>
<ul>
<li>词袋模型<br>  <img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/018bf558-4642-4d7e-b341-63a6202a3a72/截屏2022-01-26_下午1.48.32.png" alt="截屏2022-01-26 下午1.48.32.png"></li>
</ul>
<h2 id="3-2-线性分类模型"><a href="#3-2-线性分类模型" class="headerlink" title="3.2 线性分类模型"></a>3.2 线性分类模型</h2><p>线性模型<br>线性回归模型</p>
<script type="math/tex; mode=display">
f(\boldsymbol x;\boldsymbol w,b)=\boldsymbol w^T\boldsymbol x+b</script><p>线性分类模型<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6f31f9b8-156e-42bd-aa62-3b16f36a3b1b/截屏2022-01-26_下午1.57.30.png" alt="截屏2022-01-26 下午1.57.30.png"><br>二分类<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c1b4a0c2-0b8b-467d-b86e-219d68408db0/截屏2022-01-26_下午1.58.16.png" alt="截屏2022-01-26 下午1.58.16.png"><br>线性分类模型=线性判别函数+线性决策边界<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4f9c91f7-879f-4083-aae6-49a34671a9bf/截屏2022-01-26_下午2.08.32.png" alt="截屏2022-01-26 下午2.08.32.png"><br>多分类<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/79aa1bfb-3025-449e-a039-633ca7ffb955/截屏2022-01-26_下午2.07.18.png" alt="截屏2022-01-26 下午2.07.18.png"><br>a、b：投票（可能票数相等，就是问号）<br>c：打分（一定能分到一类里）</p>
<h2 id="3-3-交叉熵与对数似然"><a href="#3-3-交叉熵与对数似然" class="headerlink" title="3.3 交叉熵与对数似然"></a>3.3 交叉熵与对数似然</h2><h3 id="3-3-1-概念"><a href="#3-3-1-概念" class="headerlink" title="3.3.1 概念"></a>3.3.1 概念</h3><p>熵：随机变量X的自信息的数学期望，$H(X)=\mathbb E<em>X[I(x)]=-\Sigma</em>{x\in X}p(x)\log p(x)$<br>自信息：一个随机事件所包含的信息量，$I(x)=-\log p(x)$，可加性<br>熵编码：理论上最优的平均编码长度，用自信息编码，信息量更小，则用更小的编码长度<br>交叉熵：按照概率分布q的最优编码对真实分布为q的信息进行编码的长度$H(p,q)=\mathbb E_p[-\log q(x)]=-\Sigma_x p(x)\log q(x)$，可以用来衡量两个分布差异<br>KL散度：用概率分布q来近似p是所造成的信息损失量$KL(p,q)=H(p,q)-H(p)$</p>
<h3 id="3-3-2-应用到机器学习"><a href="#3-3-2-应用到机器学习" class="headerlink" title="3.3.2 应用到机器学习"></a>3.3.2 应用到机器学习</h3><p>以分类为例：<br>真实分布$p<em>r(y|\boldsymbol x)$，预测分布$p</em>\theta (y|\boldsymbol x)$<br>衡量两个分布差异：<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/815dc84b-ae25-4280-9e62-8e9993392073/截屏2022-01-26_下午3.01.32.png" alt="截屏2022-01-26 下午3.01.32.png"></p>
<h2 id="3-4-Logistic回归（对率回归）"><a href="#3-4-Logistic回归（对率回归）" class="headerlink" title="3.4 Logistic回归（对率回归）"></a>3.4 Logistic回归（对率回归）</h2><p>将分类问题看作条件概率估计问题<br>以二分问题为例，$p<em>\theta(y=1|\boldsymbol x)=g(f(\boldsymbol x;\boldsymbol w))$<br>判别函数f，激活函数g<br>Logistic函数：$\sigma(x)=\frac{1}{1+\exp(-x)}$<br>![截屏2022-01-26 下午3.10.24.png](<a target="_blank" rel="noopener" href="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6f4ee2bf-7cb4-4ab7-a845-9f87d844baee/截屏2022-01-26">https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6f4ee2bf-7cb4-4ab7-a845-9f87d844baee/截屏2022-01-26</a></em>下午3.10.24.png)<br>Logistic回归<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d6bd6bb1-ffa2-4e6b-b33f-fc44cfa2c270/截屏2022-01-26_下午3.10.48.png" alt="截屏2022-01-26 下午3.10.48.png"><br>学习准则<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ec353c9e-700f-4477-8c9f-5beca6457f7b/截屏2022-01-26_下午3.14.23.png" alt="截屏2022-01-26 下午3.14.23.png"></p>
<h2 id="3-5-Softmax回归"><a href="#3-5-Softmax回归" class="headerlink" title="3.5 Softmax回归"></a>3.5 Softmax回归</h2><p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/72317b9b-5ce0-4451-8b1f-47c12d282af4/截屏2022-01-26_下午5.09.03.png" alt="截屏2022-01-26 下午5.09.03.png"></p>
<script type="math/tex; mode=display">
softmax(x_k)=\frac{\exp(x_k)}{\sum_{i=1}^K\exp(x_i)}</script><p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a63a3f3b-76cf-43f6-8d07-928d88243080/截屏2022-01-26_下午5.21.40.png" alt="截屏2022-01-26 下午5.21.40.png"><br>$\boldsymbol 1_C$表示一维的全1向量，相当于求和<br>交叉熵损失函数：$-\boldsymbol y^T\log(\hat{\boldsymbol y})$ 其中$\boldsymbol y$是$[0,0,…,1,…,0]\top$的向量，$\hat{\boldsymbol y}$是用softmax算出来的向量<br>问题：softmax回归和logistic回归区别</p>
<h2 id="3-6-感知器"><a href="#3-6-感知器" class="headerlink" title="3.6 感知器"></a>3.6 感知器</h2><p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4ba697b1-3d21-4a3c-8c8c-6981ade42dd5/截屏2022-01-26_下午5.35.34.png" alt="截屏2022-01-26 下午5.35.34.png"><br>左下角：小于0就是分错了，大于0就是分对了<br>错误驱动：犯错时，用这个样本更新权重<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d9ae9ab0-a510-451a-a820-f2829bc180e1/截屏2022-01-26_下午5.38.24.png" alt="截屏2022-01-26 下午5.38.24.png"><br>反推感知器的损失函数为：$\mathcal{L}(\boldsymbol w;\boldsymbol x.y)=\max(0,-y\boldsymbol w^\top\boldsymbol x)$<br>对比Logistic回归：感知器只要犯错就更新，而Logistic回归根据犯错大小更新<br>感知器：收敛性<br>思考题：如何将感知器算法扩展到多分类问题？</p>
<h2 id="3-7-支持向量机（SVM）"><a href="#3-7-支持向量机（SVM）" class="headerlink" title="3.7 支持向量机（SVM）"></a>3.7 支持向量机（SVM）</h2><p>线性分类：“更好的”分类器<br>间隔：决策边界到分类样本的最短距离<br>支持向量机：选择间隔最大的决策边界<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3e5c35ae-a338-481a-8b72-20f57812e4a9/截屏2022-01-26_下午5.53.15.png" alt="截屏2022-01-26 下午5.53.15.png"><br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a428bd44-6289-49ec-a0f2-42bce7aa2d8c/截屏2022-01-26_下午6.11.29.png" alt="截屏2022-01-26 下午6.11.29.png"><br>软间隔：<br>松弛变量：容忍部分不满足线性约束的样本<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e93b75c4-162b-4305-9c8d-27163f241d43/截屏2022-01-26_下午6.29.16.png" alt="截屏2022-01-26 下午6.29.16.png"><br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/04e18b08-4f1b-4ce8-be35-e5f0b7e8cf17/截屏2022-01-26_下午6.33.25.png" alt="截屏2022-01-26 下午6.33.25.png"><br> Hinge损失函数对比感知器损失函数：<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ed985810-b44f-4c6e-933b-b7101d10bf03/截屏2022-01-26_下午6.34.06.png" alt="截屏2022-01-26 下午6.34.06.png"><br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f4aace8d-1649-488c-8b54-bbc74f8808b7/截屏2022-01-26_下午6.34.45.png" alt="截屏2022-01-26 下午6.34.45.png"><br>对于最上面红色那个点，感知器分对了不受惩罚，但是支持向量机由于没有在浅绿色线里面，依旧进行惩罚<br>优化：约束优化问题、SMO算法、梯度下降<br>可以和核方法结合</p>
<h2 id="3-8-线性分类模型小结"><a href="#3-8-线性分类模型小结" class="headerlink" title="3.8 线性分类模型小结"></a>3.8 线性分类模型小结</h2><p>不同损失函数对比<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4555b64f-1914-4a4a-8f64-2f48ad1882cc/截屏2022-01-26_下午6.42.32.png" alt="截屏2022-01-26 下午6.42.32.png"><br>平方损失不适合用作线性分类因为x为正时即使分对了，x越大损失越大<br>Logistic即使分对了也有损失<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fe1a6084-e610-4f8c-8cde-fd9c27b789f8/截屏2022-01-26_下午6.45.02.png" alt="截屏2022-01-26 下午6.45.02.png"><br>XOR问题<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6044162e-1b54-401c-8e7e-9ac7e61160aa/截屏2022-01-26_下午6.46.32.png" alt="截屏2022-01-26 下午6.46.32.png"><br>使用基函数的广义线性模型<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e08d3d42-dd4c-4c63-a4bd-10eb4017621a/截屏2022-01-26_下午6.47.49.png" alt="截屏2022-01-26 下午6.47.49.png"><br>比如距离</p>
<h1 id="第四讲-前馈神经网络"><a href="#第四讲-前馈神经网络" class="headerlink" title="第四讲 前馈神经网络"></a>第四讲 前馈神经网络</h1><h2 id="4-1-神经元"><a href="#4-1-神经元" class="headerlink" title="4.1 神经元"></a>4.1 神经元</h2><h3 id="4-1-1-激活函数性质"><a href="#4-1-1-激活函数性质" class="headerlink" title="4.1.1 激活函数性质"></a>4.1.1 <strong>激活函数性质</strong></h3><p>连续可导<br>激活函数及其导数要尽可能简单<br>激活函数导数值域在合适区间内（效率、稳定性）<br>单调递增（越大越兴奋）（但是这个不一定）</p>
<h3 id="4-1-2-常见激活函数"><a href="#4-1-2-常见激活函数" class="headerlink" title="4.1 2 常见激活函数"></a>4.1 2 <strong>常见激活函数</strong></h3><p><strong>S型函数</strong><br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0dc95cd6-880b-424e-b35e-2eb956d22678/截屏2022-01-27_下午4.18.19.png" alt="截屏2022-01-27 下午4.18.19.png"><br> 方法：1.normalization  2.$\sigma(x)+b$<br><strong>斜坡函数</strong><br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3c39f648-9f6e-44eb-8416-5010c6069432/截屏2022-01-27_下午4.28.31.png" alt="截屏2022-01-27 下午4.28.31.png"><br><strong>复合函数</strong></p>
<ul>
<li>Switch函数<br>  自门控（Self-Gated）激活函数<br>  <img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6cb97203-7a54-4e67-aa99-51b44e5bf06c/截屏2022-01-27_下午4.33.57.png" alt="截屏2022-01-27 下午4.33.57.png"><br>  其中$\sigma(\beta x)$为自门控，控制能通过的数据量，而自表示$\sigma(\beta x)$的大小是由x本身决定的<script type="math/tex; mode=display">
  swish(x)=x\sigma(\beta x)</script>  变换$\beta$使函数在ReLU和线性之间变换 </li>
<li>高斯误差线性单元（GELU）：<br>  <img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f2b7fbaa-6126-4182-8dc0-dce6e652a7ab/截屏2022-01-27_下午5.08.49.png" alt="截屏2022-01-27 下午5.08.49.png"><br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d6328e2d-47b5-4ed8-bf50-76361dc297c6/截屏2022-01-27_下午5.11.35.png" alt="截屏2022-01-27 下午5.11.35.png"></li>
</ul>
<h2 id="4-2-神经网络"><a href="#4-2-神经网络" class="headerlink" title="4.2 神经网络"></a>4.2 神经网络</h2><p>人工神经网络：神经元激活规则、网络拓扑结构、学习算法<br>网络结构：（方形：一组神经元）<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/27792003-9b23-4867-84a2-393b8e526944/截屏2022-01-27_下午5.14.14.png" alt="截屏2022-01-27 下午5.14.14.png"></p>
<h2 id="4-3-前馈神经网络"><a href="#4-3-前馈神经网络" class="headerlink" title="4.3 前馈神经网络"></a>4.3 前馈神经网络</h2><p>（全连接神经网络、多层感知机）<br>有向无环图<br>下图一共三层的网络<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b54be2e1-f94e-441f-a17f-82da079f3cf1/截屏2022-01-27_下午5.19.48.png" alt="截屏2022-01-27 下午5.19.48.png"><br>前馈神经网络传播信息：</p>
<script type="math/tex; mode=display">
\boldsymbol z^{(l)}=\boldsymbol W^{(l)}\boldsymbol \alpha^{(l-1)}+\boldsymbol b^{(l)}\\
\boldsymbol \alpha^{(l)}=f_l(\boldsymbol z^{(l)})</script><p>根据通用近似定理有结论：神经网络可以近似任何函数<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6dbb6ee9-ab15-46c2-aa76-2042824170d1/截屏2022-01-27_下午5.26.36.png" alt="截屏2022-01-27 下午5.26.36.png"></p>
<h2 id="4-4-反向传播算法"><a href="#4-4-反向传播算法" class="headerlink" title="4.4 反向传播算法"></a>4.4 反向传播算法</h2><p>矩阵微积分<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b98915ad-e73d-49c3-a75f-c215e00de0fa/截屏2022-01-27_下午7.52.41.png" alt="截屏2022-01-27 下午7.52.41.png"></p>
<h2 id="4-5-计算图与自动微分"><a href="#4-5-计算图与自动微分" class="headerlink" title="4.5 计算图与自动微分"></a>4.5 计算图与自动微分</h2><p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9625b520-a84a-4b52-8f8a-560c85398087/截屏2022-01-27_下午8.05.41.png" alt="截屏2022-01-27 下午8.05.41.png"><br>需要求偏导数就把路过的偏导数都乘起来<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/83d81c83-b13e-4c6c-aaeb-bd5fa86327de/截屏2022-01-27_下午8.06.47.png" alt="截屏2022-01-27 下午8.06.47.png"><br>通常使用反向模式<br>静态计算图：编译时构建计算图，效率高，可以优化，但灵活性差。tensorflow<br>动态计算图：程序运行时动态构建，灵活性好，pytorch</p>
<h2 id="4-6-优化问题"><a href="#4-6-优化问题" class="headerlink" title="4.6 优化问题"></a>4.6 优化问题</h2><p>非凸优化问题：非全局最优解，优化比较困难<br>梯度小时问题：连乘一些0-1的导数，结果就会很小，下层参数比较难调</p>
<h1 id="第五章-卷积神经网络（CNN）"><a href="#第五章-卷积神经网络（CNN）" class="headerlink" title="第五章 卷积神经网络（CNN）"></a>第五章 卷积神经网络（CNN）</h1><p>全连接神经网络：<br>权重矩阵参数非常多<br>局部不变特征（平移旋转等操作不改变语义信息，全连接前馈网络很难提取局部不变特征）<br>卷积神经网络：<br>前馈神经网络、感受野<br>局部连接、权重共享、时间或空间上的次采样</p>
<h2 id="5-1-卷积"><a href="#5-1-卷积" class="headerlink" title="5.1 卷积"></a>5.1 卷积</h2><p>计算信号的延迟累积<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5c4f17fa-7bf5-447e-a244-0a1c0e0ee6c8/截屏2022-01-28_下午1.44.09.png" alt="截屏2022-01-28 下午1.44.09.png"><br>卷积的作用：</p>
<ul>
<li>近似微分<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/35e91ed2-8258-4b9f-94d4-bfc5830da6bb/截屏2022-01-28_下午1.51.01.png" alt="截屏2022-01-28 下午1.51.01.png"></li>
<li>低通滤波、高通滤波<br>卷积扩展：引入滑动步长S和零填充P<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/dc0df44e-7a8f-4e0e-bdbe-639a6581a1b8/截屏2022-01-28_下午2.00.06.png" alt="截屏2022-01-28 下午2.00.06.png"><br>二维卷积<br>可以作为特征提取<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ed2a0d80-b8c8-41e6-8c1d-24ca8a5ad8fe/截屏2022-01-28_下午2.04.11.png" alt="截屏2022-01-28 下午2.04.11.png"><br>将卷积核作为参数让神经网络学习</li>
</ul>
<h2 id="5-2-卷积神经网络"><a href="#5-2-卷积神经网络" class="headerlink" title="5.2 卷积神经网络"></a>5.2 卷积神经网络</h2><p>卷积层代替全连接层<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0d3dbcd5-b317-4362-9482-e34669534d1f/截屏2022-01-28_下午2.06.37.png" alt="截屏2022-01-28 下午2.06.37.png"><br>一个卷积层的w只有三个参数，大幅压缩参数<br>互相关：卷积核翻转<br>卷积核看作特征提取器<br>增强卷积层能力：引入多个卷积核<br>特征映射（Feature Map）：图像经过卷积后得到的特征<br>卷积层：三维结构<br>汇聚层（Pooling）：卷积层可以显著减少连接个数，但神经元个数没有减少<br>卷积网络结构：<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/23758915-dcc6-42e7-a8a9-5d7099f0c3dd/截屏2022-01-28_下午2.19.33.png" alt="截屏2022-01-28 下午2.19.33.png"><br>趋向于小卷积、大深度，全卷积（去掉汇聚层）<br>随着深度变深，感受野会变大，更有可能提取到高级特征<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e83bf90a-4161-4e7f-ab37-f01cb28cb05c/截屏2022-01-28_下午2.21.34.png" alt="截屏2022-01-28 下午2.21.34.png"></p>
<h2 id="5-3-其他卷积种类"><a href="#5-3-其他卷积种类" class="headerlink" title="5.3 其他卷积种类"></a>5.3 其他卷积种类</h2><h3 id="空洞卷积"><a href="#空洞卷积" class="headerlink" title="空洞卷积"></a>空洞卷积</h3><p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1238e591-7560-4256-8ab9-e675c88bfa75/截屏2022-01-28_下午2.23.33.png" alt="截屏2022-01-28 下午2.23.33.png"><br>转置卷积（微步卷积）<br>低维特征映射到高维特征<br><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/51eff151-6a04-4914-9763-4462dbf5188a/截屏2022-01-28_下午2.25.46.png" alt="截屏2022-01-28 下午2.25.46.png"></p>
<h2 id="5-4-经典卷积网络"><a href="#5-4-经典卷积网络" class="headerlink" title="5.4 经典卷积网络"></a>5.4 经典卷积网络</h2><h2 id="5-5-卷积网络的应用"><a href="#5-5-卷积网络的应用" class="headerlink" title="5.5 卷积网络的应用"></a>5.5 卷积网络的应用</h2><h2 id="5-6-应用到文本数据"><a href="#5-6-应用到文本数据" class="headerlink" title="5.6 应用到文本数据"></a>5.6 应用到文本数据</h2><p>文本序列的卷积模型</p>
<h1 id="第六章-循环神经网络"><a href="#第六章-循环神经网络" class="headerlink" title="第六章 循环神经网络"></a>第六章 循环神经网络</h1><h2 id="6-1-给神经网络增加记忆能力"><a href="#6-1-给神经网络增加记忆能力" class="headerlink" title="6.1 给神经网络增加记忆能力"></a>6.1 给神经网络增加记忆能力</h2><p>前馈网络：相邻两层单向连接、层内无连接，有向无环图，输入输出维数固定</p>
<p>图灵机：纸带、读写头</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/efe6d951-4964-4618-88c5-a023b71d4982/截屏2022-01-28_下午3.21.18.png" alt="截屏2022-01-28 下午3.21.18.png"></p>
<h3 id="时延神经网络（TDNN）"><a href="#时延神经网络（TDNN）" class="headerlink" title="时延神经网络（TDNN）"></a>时延神经网络（TDNN）</h3><p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fe20e161-3850-49d6-86ed-04d7b87019b0/截屏2022-01-28_下午3.24.10.png" alt="截屏2022-01-28 下午3.24.10.png"></p>
<p>短期记忆能力</p>
<h3 id="自回归模型（AR）"><a href="#自回归模型（AR）" class="headerlink" title="自回归模型（AR）"></a>自回归模型（AR）</h3><p>用历史信息预测自己，但没有外部输入</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3f47959d-5721-4d33-b8db-c6e9de81cab8/截屏2022-01-28_下午3.26.07.png" alt="截屏2022-01-28 下午3.26.07.png"></p>
<h3 id="有外部输入的非线性自回归模型（NARX）"><a href="#有外部输入的非线性自回归模型（NARX）" class="headerlink" title="有外部输入的非线性自回归模型（NARX）"></a>有外部输入的非线性自回归模型（NARX）</h3><p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1552232d-04ba-47d3-b051-8d6e5be8bc70/截屏2022-01-28_下午3.27.05.png" alt="截屏2022-01-28 下午3.27.05.png"></p>
<p>f表示非线性函数，可以是前馈网络</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9afc56ea-7304-49d9-ab47-68ececa545ab/截屏2022-01-28_下午3.27.52.png" alt="截屏2022-01-28 下午3.27.52.png"></p>
<h2 id="6-2-循环神经网络（RNN）"><a href="#6-2-循环神经网络（RNN）" class="headerlink" title="6.2 循环神经网络（RNN）"></a>6.2 循环神经网络（RNN）</h2><p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2214601c-ead2-46cf-a7b7-08ca3f39ce04/截屏2022-01-28_下午3.44.32.png" alt="截屏2022-01-28 下午3.44.32.png"></p>
<p>使用带自反馈的神经元，处理任意长度的时序数据</p>
<p>时间维上很深，但非时间维上很浅</p>
<h3 id="简单循环网络（SRN）"><a href="#简单循环网络（SRN）" class="headerlink" title="简单循环网络（SRN）"></a>简单循环网络（SRN）</h3><p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8e3b4290-b32b-4bb9-bcec-b9c0836d6f5a/截屏2022-01-28_下午3.45.09.png" alt="截屏2022-01-28 下午3.45.09.png"></p>
<p>一个完全连接的循环网络是任何非线性动力系统的近似器</p>
<p>完全连接的循环神经网络是图灵完备（解决所有的可计算问题）的</p>
<h3 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h3><p>输入输出映射（机器学习模型）</p>
<p>存储器</p>
<h2 id="6-3-应用到机器学习"><a href="#6-3-应用到机器学习" class="headerlink" title="6.3 应用到机器学习"></a>6.3 应用到机器学习</h2><h3 id="序列到类别"><a href="#序列到类别" class="headerlink" title="序列到类别"></a>序列到类别</h3><p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e7e5a3a1-7429-41eb-a030-f5ca8311385f/截屏2022-01-28_下午4.05.32.png" alt="截屏2022-01-28 下午4.05.32.png"></p>
<ul>
<li><p>情感分类</p>
<p>  <img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/123516b3-7bff-4d07-a204-d0a31122c3d9/截屏2022-01-28_下午4.07.04.png" alt="截屏2022-01-28 下午4.07.04.png"></p>
</li>
</ul>
<h3 id="同步的序列到序列模式"><a href="#同步的序列到序列模式" class="headerlink" title="同步的序列到序列模式"></a>同步的序列到序列模式</h3><p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/7b6b96e6-b6c6-4a6c-93bb-c7d8b0bf6d0d/截屏2022-01-28_下午4.08.07.png" alt="截屏2022-01-28 下午4.08.07.png"></p>
<ul>
<li><p>中文分词</p>
<p>  <img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ca8d8928-43f5-49bc-83ed-874f47f4a0b5/截屏2022-01-28_下午4.10.21.png" alt="截屏2022-01-28 下午4.10.21.png"></p>
</li>
<li><p>信息抽取</p>
<p>  从无结构文本抽取结构化信息，形成知识</p>
<p>  <img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e3d74abb-2887-474c-b162-282ff88a4560/截屏2022-01-28_下午4.12.59.png" alt="截屏2022-01-28 下午4.12.59.png"></p>
</li>
<li><p>语音识别</p>
<p>  <img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/80623b51-306c-4895-9131-c37ec73c6b58/截屏2022-01-28_下午4.12.42.png" alt="截屏2022-01-28 下午4.12.42.png"></p>
</li>
</ul>
<h3 id="异步的序列到序列模式"><a href="#异步的序列到序列模式" class="headerlink" title="异步的序列到序列模式"></a>异步的序列到序列模式</h3><p>Encoder只有输入没有输出</p>
<p>Decoder只有输出没有输入（自回归模型）</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/7de8ebdc-78ef-4d66-b120-da8d2fc7a6cb/截屏2022-01-28_下午4.15.30.png" alt="截屏2022-01-28 下午4.15.30.png"></p>
<ul>
<li><p>机器翻译</p>
<p>  <img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/016fc21f-3a8d-4617-a823-b69113430d01/截屏2022-01-28_下午4.16.43.png" alt="截屏2022-01-28 下午4.16.43.png"></p>
</li>
</ul>
<h2 id="6-4-参数学习与长程依赖问题"><a href="#6-4-参数学习与长程依赖问题" class="headerlink" title="6.4 参数学习与长程依赖问题"></a>6.4 参数学习与长程依赖问题</h2><h3 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h3><p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/03b7494c-2cdd-4929-90d0-cddcacfbc29c/截屏2022-01-28_下午4.23.10.png" alt="截屏2022-01-28 下午4.23.10.png"></p>
<h3 id="长程依赖问题"><a href="#长程依赖问题" class="headerlink" title="长程依赖问题"></a>长程依赖问题</h3><p>由于梯度消失或者梯度爆炸问题，只能学习到短周期的依赖关系</p>
<script type="math/tex; mode=display">
\frac{\partial L_t}{\partial U^k } \rightarrow 0/+\infty, t-k\rightarrow \infty</script><h2 id="6-5-如何解决长程依赖问题"><a href="#6-5-如何解决长程依赖问题" class="headerlink" title="6.5 如何解决长程依赖问题"></a>6.5 如何解决长程依赖问题</h2><h3 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h3><ul>
<li>权重衰减</li>
<li>梯度截断</li>
</ul>
<h3 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h3><ul>
<li>改进模型</li>
</ul>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8193a56d-a37e-4dde-a0c7-8a8db4096210/截屏2022-01-28_下午4.43.09.png" alt="截屏2022-01-28 下午4.43.09.png"></p>
<p>（残差网络）</p>
<h2 id="6-6-GRU和LSTM"><a href="#6-6-GRU和LSTM" class="headerlink" title="6.6 GRU和LSTM"></a>6.6 GRU和LSTM</h2><h3 id="门控机制"><a href="#门控机制" class="headerlink" title="门控机制"></a>门控机制</h3><p>有选择地加入新信息，有选择地遗忘之前累积的信息</p>
<p>基于门控的循环神经网络</p>
<ul>
<li>门控循环单元（GRU）</li>
<li>长短期记忆网络（LSTM）</li>
</ul>
<h3 id="门控循环单元（GRU）"><a href="#门控循环单元（GRU）" class="headerlink" title="门控循环单元（GRU）"></a>门控循环单元（GRU）</h3><p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/28278475-3541-41b1-9d01-d33eb18f604b/截屏2022-01-28_下午4.52.36.png" alt="截屏2022-01-28 下午4.52.36.png"></p>
<p>$r<em>t$控制要不要$h</em>{t-1}$</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d9b9acf2-32af-4b5c-abb5-b49854bf06e3/截屏2022-01-28_下午4.58.40.png" alt="截屏2022-01-28 下午4.58.40.png"></p>
<h3 id="长短期记忆神经网络（LSTM）"><a href="#长短期记忆神经网络（LSTM）" class="headerlink" title="长短期记忆神经网络（LSTM）"></a>长短期记忆神经网络（LSTM）</h3><p> 引入c进行线性关系，释放h，提高网络非线性建模能力</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/476a79cf-92ef-49d7-85f0-9e326a784032/截屏2022-01-28_下午5.04.53.png" alt="截屏2022-01-28 下午5.04.53.png"></p>
<h2 id="6-7-深层循环神经网络"><a href="#6-7-深层循环神经网络" class="headerlink" title="6.7 深层循环神经网络"></a>6.7 深层循环神经网络</h2><h3 id="堆叠循环神经网络"><a href="#堆叠循环神经网络" class="headerlink" title="堆叠循环神经网络"></a>堆叠循环神经网络</h3><p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b1976804-4a2d-487d-aaa9-b3edfc9da07d/截屏2022-01-28_下午5.08.18.png" alt="截屏2022-01-28 下午5.08.18.png"></p>
<h3 id="双向循环神经网络"><a href="#双向循环神经网络" class="headerlink" title="双向循环神经网络"></a>双向循环神经网络</h3><p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a945fb8b-d9c8-40fa-8ba3-995e4c1cd41c/截屏2022-01-28_下午5.09.01.png" alt="截屏2022-01-28 下午5.09.01.png"></p>
<p>循环网络小结：</p>
<p>优点：引入（短期）记忆、图灵完备</p>
<p>缺点：长程依赖问题、记忆容量问题、并行能力</p>
<h2 id="6-8-循环网络应用"><a href="#6-8-循环网络应用" class="headerlink" title="6.8 循环网络应用"></a>6.8 循环网络应用</h2><h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><p>语义、语法——概率</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/02e30255-9ca5-4c21-a902-1c8052f605e2/截屏2022-01-28_下午5.18.06.png" alt="截屏2022-01-28 下午5.18.06.png"></p>
<p>写字、作诗、对话系统等</p>
<h2 id="6-9-扩展到图结构"><a href="#6-9-扩展到图结构" class="headerlink" title="6.9 扩展到图结构"></a>6.9 扩展到图结构</h2><h3 id="序列：循环神经网络"><a href="#序列：循环神经网络" class="headerlink" title="序列：循环神经网络"></a>序列：循环神经网络</h3><h3 id="树：递归神经网络"><a href="#树：递归神经网络" class="headerlink" title="树：递归神经网络"></a>树：递归神经网络</h3><p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a35493c2-897f-4733-9829-fe5b09b5aa0d/截屏2022-01-28_下午5.26.12.png" alt="截屏2022-01-28 下午5.26.12.png"></p>
<h3 id="图：图网络"><a href="#图：图网络" class="headerlink" title="图：图网络"></a>图：图网络</h3><h1 id="第七章-网络优化与正则化"><a href="#第七章-网络优化与正则化" class="headerlink" title="第七章 网络优化与正则化"></a>第七章 网络优化与正则化</h1><p>优化：经验风险最小</p>
<p>正则化：降低模型复杂度</p>
<h2 id="7-1-神经网络优化特点"><a href="#7-1-神经网络优化特点" class="headerlink" title="7.1 神经网络优化特点"></a>7.1 神经网络优化特点</h2><p>结构差异大：没有通用算法、超参数多</p>
<p>非凸优化问题：参数初始化、逃离局部最优或鞍点</p>
<p>梯度消失（爆炸）问题</p>
<p>低维空间的非凸优化：逃离局部最优点</p>
<p>高维空间的非凸优化：</p>
<ul>
<li>大部分梯度为0的点都是鞍点，逃离鞍点</li>
<li>平坦最小值（不一定要找全局最小点）</li>
</ul>
<p>优化地形：高维空间中的损失函数的曲面形状</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/58f3ee42-a79f-4209-947a-e3e96bc4b8e6/截屏2022-01-29_下午4.15.32.png" alt="截屏2022-01-29 下午4.15.32.png"></p>
<p>添加残差连接后更为平滑</p>
<p>神经网络优化的改善方法</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6cd95c69-9187-4773-81ef-fc36f4492965/截屏2022-01-29_下午4.16.11.png" alt="截屏2022-01-29 下午4.16.11.png"></p>
<h2 id="7-2-优化算法改进"><a href="#7-2-优化算法改进" class="headerlink" title="7.2 优化算法改进"></a>7.2 优化算法改进</h2><p>批量大小</p>
<p>不影响随机梯度的期望，但是影响随机梯度的方差</p>
<p>批量越大，方差越小，噪声越小，训练稳定，可以设置较大学习率</p>
<p>批量大小和学习率：线性缩放规则 </p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3482ffb7-947b-4609-badb-108ffb8de48a/截屏2022-01-29_下午5.20.28.png" alt="截屏2022-01-29 下午5.20.28.png"></p>
<p>批量大的时候按iteration来看，可以看到更多样本，更快收敛</p>
<p>批量小的时候按epoch来看，走的步长更多，更能找到最优的路径</p>
<p>批量小的时候随机性更大</p>
<p>算力：GPU能放多少放多少，越大计算效率高，但迭代效率越慢，相当于取折中</p>
<p>泛化：批量越小随机性越强，充当正则化效应</p>
<p>训练样本足够多通常就是GPU放满，样本不够多时批量小比较好</p>
<p>学习率、梯度动态变化</p>
<h2 id="7-3-动态学习率"><a href="#7-3-动态学习率" class="headerlink" title="7.3 动态学习率"></a>7.3 动态学习率</h2><p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ca325b68-01b2-49b4-a819-2e51cee93dbe/截屏2022-01-29_下午5.30.31.png" alt="截屏2022-01-29 下午5.30.31.png"></p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9736b983-0e39-40f4-8c84-b80a5be604f6/截屏2022-01-29_下午5.33.08.png" alt="截屏2022-01-29 下午5.33.08.png"></p>
<p>周期性学习率调整（跳出局部最优）</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1bfcc5c8-a944-4f26-a395-c22309a807cc/截屏2022-01-29_下午5.33.44.png" alt="截屏2022-01-29 下午5.33.44.png"></p>
<p>更容易走到平坦的最优，增加模型鲁棒性（不会因为参数变化导致模型性能有很大改变）</p>
<h3 id="其他调整学习率方法"><a href="#其他调整学习率方法" class="headerlink" title="其他调整学习率方法"></a>其他调整学习率方法</h3><ul>
<li>增大批量大小</li>
<li>学习率预热（最开始的梯度比较乱，用小的学习率找到比较稳定的位置）</li>
</ul>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/bc65b87c-4977-4f02-b686-bf6259d14645/截屏2022-01-29_下午5.38.29.png" alt="截屏2022-01-29 下午5.38.29.png"></p>
<ul>
<li><p>自适应学习率</p>
<p>  <img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/114fa287-215a-4167-9bee-4cdd3cc44bcd/截屏2022-01-29_下午5.41.31.png" alt="截屏2022-01-29 下午5.41.31.png"></p>
</li>
</ul>
<h1 id="第八章-注意力机制与外部记忆"><a href="#第八章-注意力机制与外部记忆" class="headerlink" title="第八章 注意力机制与外部记忆"></a>第八章 注意力机制与外部记忆</h1><h1 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h1><h2 id="8-2-人工神经网络中的注意力机制"><a href="#8-2-人工神经网络中的注意力机制" class="headerlink" title="8.2 人工神经网络中的注意力机制"></a>8.2 人工神经网络中的注意力机制</h2><h3 id="软性注意力机制"><a href="#软性注意力机制" class="headerlink" title="软性注意力机制"></a>软性注意力机制</h3><p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1521b54c-fe94-485d-a6e1-7c69d64b82a5/截屏2022-01-29_下午11.08.56.png" alt="截屏2022-01-29 下午11.08.56.png"></p>
<p>注意力打分函数</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2579229c-4e37-468c-9333-cdc722ada73e/截屏2022-01-29_下午11.09.57.png" alt="截屏2022-01-29 下午11.09.57.png"></p>
<p>双线性模型：带方向的</p>
<h3 id="硬性注意力"><a href="#硬性注意力" class="headerlink" title="硬性注意力"></a>硬性注意力</h3><p>注意力选择的时候不是通过概率（软性），而是0/1，这样就没有梯度，和强化学习结合</p>
<h3 id="键值注意力"><a href="#键值注意力" class="headerlink" title="键值注意力"></a>键值注意力</h3><p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6914430c-a5f1-4f30-b497-c256f4cfd48b/截屏2022-01-29_下午11.14.57.png" alt="截屏2022-01-29 下午11.14.57.png"></p>
<p>计算相似度用一套机制，汇总时候用另一套</p>
<h3 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h3><p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/051257eb-a473-47c1-b13c-61047be3546c/截屏2022-01-29_下午11.16.47.png" alt="截屏2022-01-29 下午11.16.47.png"></p>
<h3 id="结构化注意力"><a href="#结构化注意力" class="headerlink" title="结构化注意力"></a>结构化注意力</h3><h3 id="指针网络"><a href="#指针网络" class="headerlink" title="指针网络"></a>指针网络</h3><p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a5c20dcd-d107-491f-92b3-558de0e994fb/截屏2022-01-29_下午11.20.23.png" alt="截屏2022-01-29 下午11.20.23.png"></p>
<h1 id="增强记忆网络"><a href="#增强记忆网络" class="headerlink" title="增强记忆网络"></a>增强记忆网络</h1>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/08/08/%E8%AE%BA-hdu-2973-YAPTCHA/" rel="next" title="[数论]hdu 2973 YAPTCHA">
                <i class="fa fa-chevron-left"></i> [数论]hdu 2973 YAPTCHA
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2022/02/17/%E5%A7%97%E5%A7%97%E6%9D%A5%E8%BF%9F%E7%9A%84%E9%80%80%E5%BD%B9%E8%B4%B4/" rel="prev" title="姗姗来迟的退役贴">
                姗姗来迟的退役贴 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/header.jpg"
                alt="" />
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/">
              
                  <span class="site-state-item-count">28</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友链
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://zzy991212.github.io/" title="zzy" target="_blank">zzy</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="blog.kongjune.com" title="KONGJUNE" target="_blank">KONGJUNE</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="www.zhouwk.club" title="zhouwk" target="_blank">zhouwk</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.cnblogs.com/conver/" title="conver" target="_blank">conver</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://vincentxwd.github.io/blog/" title="Kirai" target="_blank">Kirai</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E7%AB%A0-%E7%BB%AA%E8%AE%BA"><span class="nav-number">1.</span> <span class="nav-text">第一章 绪论</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 人工智能</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-1-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E5%8F%91%E5%B1%95%E5%8E%86%E5%8F%B2"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1.1 人工智能的发展历史</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-2-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E6%B5%81%E6%B4%BE"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.1.2 人工智能的流派</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88ML%EF%BC%89"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 机器学习（ML）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 表示学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-1-%E5%B1%80%E9%83%A8%E8%A1%A8%E7%A4%BA%E5%92%8C%E5%88%86%E5%B8%83%E5%BC%8F%E8%A1%A8%E7%A4%BA"><span class="nav-number">1.3.1.</span> <span class="nav-text">1.3.1 局部表示和分布式表示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-2-%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.3.2.</span> <span class="nav-text">1.3.2 表示学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%88DL%EF%BC%89"><span class="nav-number">1.4.</span> <span class="nav-text">1.4 深度学习（DL）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-5-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.5.</span> <span class="nav-text">1.5 神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-1-%E4%BA%BA%E8%84%91%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.5.1.</span> <span class="nav-text">1.5.1 人脑神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-2-%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.5.2.</span> <span class="nav-text">1.5.2 人工神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-3-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8F%91%E5%B1%95%E5%8E%86%E5%8F%B2"><span class="nav-number">1.5.3.</span> <span class="nav-text">1.5.3 神经网络的发展历史</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-6-%E6%9C%AC%E4%B9%A6%E7%9A%84%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB"><span class="nav-number">1.6.</span> <span class="nav-text">1.6 本书的知识体系</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0"><span class="nav-number">2.</span> <span class="nav-text">第二章 机器学习概述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 基本概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%89%E4%B8%AA%E5%9F%BA%E6%9C%AC%E8%A6%81%E7%B4%A0"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 机器学习的三个基本要素</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.2.1 模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-%E5%AD%A6%E4%B9%A0%E5%87%86%E5%88%99"><span class="nav-number">2.2.2.</span> <span class="nav-text">2.2.2 学习准则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-3-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="nav-number">2.2.3.</span> <span class="nav-text">2.2.3 优化算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%AE%80%E5%8D%95%E7%A4%BA%E4%BE%8B%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 机器学习的简单示例——线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%8E%E5%87%BD%E6%95%B0%E5%85%B3%E7%B3%BB%E7%9C%8B"><span class="nav-number">2.3.1.</span> <span class="nav-text">从函数关系看</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%9B%B2%E7%BA%BF%E6%8B%9F%E5%90%88"><span class="nav-number">2.4.</span> <span class="nav-text">多项式曲线拟合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-%E5%81%8F%E5%B7%AE-%E6%96%B9%E5%B7%AE%E5%88%86%E8%A7%A3"><span class="nav-number">2.5.</span> <span class="nav-text">2.4 偏差-方差分解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-5-%E5%AE%9A%E7%90%86"><span class="nav-number">2.6.</span> <span class="nav-text">2.5 定理</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.</span> <span class="nav-text">第三章 线性模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%A4%BA%E4%BE%8B"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 分类问题示例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 线性分类模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-%E4%BA%A4%E5%8F%89%E7%86%B5%E4%B8%8E%E5%AF%B9%E6%95%B0%E4%BC%BC%E7%84%B6"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 交叉熵与对数似然</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-1-%E6%A6%82%E5%BF%B5"><span class="nav-number">3.3.1.</span> <span class="nav-text">3.3.1 概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-2-%E5%BA%94%E7%94%A8%E5%88%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="nav-number">3.3.2.</span> <span class="nav-text">3.3.2 应用到机器学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-4-Logistic%E5%9B%9E%E5%BD%92%EF%BC%88%E5%AF%B9%E7%8E%87%E5%9B%9E%E5%BD%92%EF%BC%89"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 Logistic回归（对率回归）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-5-Softmax%E5%9B%9E%E5%BD%92"><span class="nav-number">3.5.</span> <span class="nav-text">3.5 Softmax回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-6-%E6%84%9F%E7%9F%A5%E5%99%A8"><span class="nav-number">3.6.</span> <span class="nav-text">3.6 感知器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-7-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89"><span class="nav-number">3.7.</span> <span class="nav-text">3.7 支持向量机（SVM）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-8-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%B0%8F%E7%BB%93"><span class="nav-number">3.8.</span> <span class="nav-text">3.8 线性分类模型小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E5%9B%9B%E8%AE%B2-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">4.</span> <span class="nav-text">第四讲 前馈神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-%E7%A5%9E%E7%BB%8F%E5%85%83"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 神经元</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-1-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%80%A7%E8%B4%A8"><span class="nav-number">4.1.1.</span> <span class="nav-text">4.1.1 激活函数性质</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-2-%E5%B8%B8%E8%A7%81%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">4.1.2.</span> <span class="nav-text">4.1 2 常见激活函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">4.3.</span> <span class="nav-text">4.3 前馈神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-4-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95"><span class="nav-number">4.4.</span> <span class="nav-text">4.4 反向传播算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-5-%E8%AE%A1%E7%AE%97%E5%9B%BE%E4%B8%8E%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86"><span class="nav-number">4.5.</span> <span class="nav-text">4.5 计算图与自动微分</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-6-%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98"><span class="nav-number">4.6.</span> <span class="nav-text">4.6 优化问题</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%BA%94%E7%AB%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89"><span class="nav-number">5.</span> <span class="nav-text">第五章 卷积神经网络（CNN）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1-%E5%8D%B7%E7%A7%AF"><span class="nav-number">5.1.</span> <span class="nav-text">5.1 卷积</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">5.2.</span> <span class="nav-text">5.2 卷积神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3-%E5%85%B6%E4%BB%96%E5%8D%B7%E7%A7%AF%E7%A7%8D%E7%B1%BB"><span class="nav-number">5.3.</span> <span class="nav-text">5.3 其他卷积种类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF"><span class="nav-number">5.3.1.</span> <span class="nav-text">空洞卷积</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-4-%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C"><span class="nav-number">5.4.</span> <span class="nav-text">5.4 经典卷积网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-5-%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E7%9A%84%E5%BA%94%E7%94%A8"><span class="nav-number">5.5.</span> <span class="nav-text">5.5 卷积网络的应用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-6-%E5%BA%94%E7%94%A8%E5%88%B0%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE"><span class="nav-number">5.6.</span> <span class="nav-text">5.6 应用到文本数据</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E5%85%AD%E7%AB%A0-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">6.</span> <span class="nav-text">第六章 循环神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#6-1-%E7%BB%99%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%A2%9E%E5%8A%A0%E8%AE%B0%E5%BF%86%E8%83%BD%E5%8A%9B"><span class="nav-number">6.1.</span> <span class="nav-text">6.1 给神经网络增加记忆能力</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%B6%E5%BB%B6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88TDNN%EF%BC%89"><span class="nav-number">6.1.1.</span> <span class="nav-text">时延神经网络（TDNN）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%EF%BC%88AR%EF%BC%89"><span class="nav-number">6.1.2.</span> <span class="nav-text">自回归模型（AR）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%89%E5%A4%96%E9%83%A8%E8%BE%93%E5%85%A5%E7%9A%84%E9%9D%9E%E7%BA%BF%E6%80%A7%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%EF%BC%88NARX%EF%BC%89"><span class="nav-number">6.1.3.</span> <span class="nav-text">有外部输入的非线性自回归模型（NARX）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-2-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88RNN%EF%BC%89"><span class="nav-number">6.2.</span> <span class="nav-text">6.2 循环神经网络（RNN）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E5%BE%AA%E7%8E%AF%E7%BD%91%E7%BB%9C%EF%BC%88SRN%EF%BC%89"><span class="nav-number">6.2.1.</span> <span class="nav-text">简单循环网络（SRN）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%9C%E7%94%A8"><span class="nav-number">6.2.2.</span> <span class="nav-text">作用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-3-%E5%BA%94%E7%94%A8%E5%88%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="nav-number">6.3.</span> <span class="nav-text">6.3 应用到机器学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BA%8F%E5%88%97%E5%88%B0%E7%B1%BB%E5%88%AB"><span class="nav-number">6.3.1.</span> <span class="nav-text">序列到类别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%8C%E6%AD%A5%E7%9A%84%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E6%A8%A1%E5%BC%8F"><span class="nav-number">6.3.2.</span> <span class="nav-text">同步的序列到序列模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%82%E6%AD%A5%E7%9A%84%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E6%A8%A1%E5%BC%8F"><span class="nav-number">6.3.3.</span> <span class="nav-text">异步的序列到序列模式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-4-%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0%E4%B8%8E%E9%95%BF%E7%A8%8B%E4%BE%9D%E8%B5%96%E9%97%AE%E9%A2%98"><span class="nav-number">6.4.</span> <span class="nav-text">6.4 参数学习与长程依赖问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0"><span class="nav-number">6.4.1.</span> <span class="nav-text">参数学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%95%BF%E7%A8%8B%E4%BE%9D%E8%B5%96%E9%97%AE%E9%A2%98"><span class="nav-number">6.4.2.</span> <span class="nav-text">长程依赖问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-5-%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E9%95%BF%E7%A8%8B%E4%BE%9D%E8%B5%96%E9%97%AE%E9%A2%98"><span class="nav-number">6.5.</span> <span class="nav-text">6.5 如何解决长程依赖问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8"><span class="nav-number">6.5.1.</span> <span class="nav-text">梯度爆炸</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1"><span class="nav-number">6.5.2.</span> <span class="nav-text">梯度消失</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-6-GRU%E5%92%8CLSTM"><span class="nav-number">6.6.</span> <span class="nav-text">6.6 GRU和LSTM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%A8%E6%8E%A7%E6%9C%BA%E5%88%B6"><span class="nav-number">6.6.1.</span> <span class="nav-text">门控机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83%EF%BC%88GRU%EF%BC%89"><span class="nav-number">6.6.2.</span> <span class="nav-text">门控循环单元（GRU）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88LSTM%EF%BC%89"><span class="nav-number">6.6.3.</span> <span class="nav-text">长短期记忆神经网络（LSTM）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-7-%E6%B7%B1%E5%B1%82%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">6.7.</span> <span class="nav-text">6.7 深层循环神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A0%86%E5%8F%A0%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">6.7.1.</span> <span class="nav-text">堆叠循环神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">6.7.2.</span> <span class="nav-text">双向循环神经网络</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-8-%E5%BE%AA%E7%8E%AF%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8"><span class="nav-number">6.8.</span> <span class="nav-text">6.8 循环网络应用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">6.8.1.</span> <span class="nav-text">语言模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-9-%E6%89%A9%E5%B1%95%E5%88%B0%E5%9B%BE%E7%BB%93%E6%9E%84"><span class="nav-number">6.9.</span> <span class="nav-text">6.9 扩展到图结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BA%8F%E5%88%97%EF%BC%9A%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">6.9.1.</span> <span class="nav-text">序列：循环神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%91%EF%BC%9A%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">6.9.2.</span> <span class="nav-text">树：递归神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%EF%BC%9A%E5%9B%BE%E7%BD%91%E7%BB%9C"><span class="nav-number">6.9.3.</span> <span class="nav-text">图：图网络</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%B8%83%E7%AB%A0-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">7.</span> <span class="nav-text">第七章 网络优化与正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#7-1-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E7%89%B9%E7%82%B9"><span class="nav-number">7.1.</span> <span class="nav-text">7.1 神经网络优化特点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-2-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E6%94%B9%E8%BF%9B"><span class="nav-number">7.2.</span> <span class="nav-text">7.2 优化算法改进</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-3-%E5%8A%A8%E6%80%81%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="nav-number">7.3.</span> <span class="nav-text">7.3 动态学习率</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E8%B0%83%E6%95%B4%E5%AD%A6%E4%B9%A0%E7%8E%87%E6%96%B9%E6%B3%95"><span class="nav-number">7.3.1.</span> <span class="nav-text">其他调整学习率方法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E5%85%AB%E7%AB%A0-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%B8%8E%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86"><span class="nav-number">8.</span> <span class="nav-text">第八章 注意力机制与外部记忆</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">9.</span> <span class="nav-text">注意力机制</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#8-2-%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">9.1.</span> <span class="nav-text">8.2 人工神经网络中的注意力机制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BD%AF%E6%80%A7%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">9.1.1.</span> <span class="nav-text">软性注意力机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A1%AC%E6%80%A7%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-number">9.1.2.</span> <span class="nav-text">硬性注意力</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%94%AE%E5%80%BC%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-number">9.1.3.</span> <span class="nav-text">键值注意力</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-number">9.1.4.</span> <span class="nav-text">多头注意力</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%93%E6%9E%84%E5%8C%96%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-number">9.1.5.</span> <span class="nav-text">结构化注意力</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8C%87%E9%92%88%E7%BD%91%E7%BB%9C"><span class="nav-number">9.1.6.</span> <span class="nav-text">指针网络</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A2%9E%E5%BC%BA%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C"><span class="nav-number">10.</span> <span class="nav-text">增强记忆网络</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">毛毛雨 =)</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
